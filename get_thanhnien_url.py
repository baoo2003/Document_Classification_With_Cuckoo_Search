# -*- coding: utf-8 -*-
import os, csv, time
from urllib.parse import urljoin
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import WebDriverException
from webdriver_manager.chrome import ChromeDriverManager


def crawl_infinite_links(
    category_url: str,
    css_selector: str = 'a.box-category-link-with-avatar.img-resize',
    output_csv: str = 'thanhnien_links.csv',
    max_scrolls: int = 60,
    pause_sec: float = 1.2,
    stop_if_no_new_rounds: int = 3,
    headless: bool = True,
    try_click_selectors: tuple = (
        'a.view-more.btn-viewmore[href="javascript:;"]',
    ),
):
    """
    Cu·ªôn v√¥ h·∫°n v√† thu th·∫≠p t·∫•t c·∫£ href theo css_selector t·ª´ category_url.
    L∆∞u CSV (append, kh√¥ng ghi ƒë√®, b·ªè qua link tr√πng). C√≥ ph√°t hi·ªán 'k·∫πt cu·ªôn'.
    """
    # --- Chrome Options (gi·∫£m b·ªã ch·∫∑n headless) ---
    chrome_opts = Options()
    if headless:
        chrome_opts.add_argument("--headless=new")
    chrome_opts.add_argument("--disable-gpu")
    chrome_opts.add_argument("--no-sandbox")
    chrome_opts.add_argument("--disable-dev-shm-usage")
    chrome_opts.add_argument("--window-size=1280,2600")
    chrome_opts.add_argument("--disable-blink-features=AutomationControlled")
    chrome_opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_opts.add_experimental_option('useAutomationExtension', False)
    chrome_opts.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    )

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=chrome_opts
    )

    # --- Load links c≈© (tr√°nh tr√πng) ---
    existing = set()
    if os.path.exists(output_csv):
        with open(output_csv, encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                existing.add(row['url'])
        print(f"üîπ Loaded {len(existing)} existing links from {output_csv}")

    collected = set()
    no_new_rounds = 0

    try:
        driver.get(category_url)
        time.sleep(2)

        # Helper: ƒë√≥ng cookie/consent n·∫øu c√≥ (best-effort)
        for sel in ['button#onetrust-accept-btn-handler', '.ot-pc-refuse-all-handler',
                    'button[aria-label="accept"]', 'button:contains("ƒê·ªìng √Ω")']:
            try:
                for el in driver.find_elements(By.CSS_SELECTOR, sel):
                    if el.is_displayed() and el.is_enabled():
                        driver.execute_script("arguments[0].click();", el)
                        time.sleep(0.5)
            except Exception:
                pass

        # Track chi·ªÅu cao trang ƒë·ªÉ bi·∫øt c√≥ c√≤n load ti·∫øp kh√¥ng
        last_height = driver.execute_script("return document.body.scrollHeight")

        for i in range(max_scrolls):
            # 1) Th·ª≠ click 'xem th√™m' n·∫øu c√≥
            for sel in try_click_selectors:
                try:
                    for btn in driver.find_elements(By.CSS_SELECTOR, sel):
                        if btn.is_displayed() and btn.is_enabled():
                            try:
                                driver.execute_script("arguments[0].click();", btn)
                                time.sleep(pause_sec)
                            except WebDriverException:
                                try:
                                    btn.click(); time.sleep(pause_sec)
                                except Exception:
                                    pass
                except Exception:
                    pass

            # 2) Cu·ªôn t·ª´ng b∆∞·ªõc nh·ªè ƒë·ªÉ k√≠ch ho·∫°t lazy load theo viewport
            for _ in range(5):
                driver.execute_script(
                    "window.scrollBy(0, Math.floor(window.innerHeight*0.9));"
                )
                time.sleep(pause_sec/3)

            # 3) Cu·ªôn t·ªõi ƒë√°y + ch·ªù
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(pause_sec)

            # 4) Ki·ªÉm tra chi·ªÅu cao trang c√≥ tƒÉng kh√¥ng
            new_height = driver.execute_script("return document.body.scrollHeight")
            height_grew = new_height > last_height
            last_height = new_height

            # 5) Parse & gom link
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            before = len(collected)
            for a in soup.select(css_selector):
                href = a.get('href')
                if not href:
                    continue
                full = urljoin(category_url, href)
                if (full not in existing) and (full not in collected):
                    collected.add(full)
            new_links = len(collected) - before

            print(f"üîπ Round {i+1}: +{new_links} new, total={len(existing) + len(collected)} | height_grew={height_grew}")

            # 6) Logic d·ª´ng: kh√¥ng c√≥ link m·ªõi trong v√≤ng n√†y **ho·∫∑c** chi·ªÅu cao kh√¥ng tƒÉng 3 v√≤ng li√™n ti·∫øp
            if new_links == 0 and not height_grew:
                no_new_rounds += 1
                if no_new_rounds >= stop_if_no_new_rounds:
                    print(f"‚ÑπÔ∏è  Stop: no progress for {no_new_rounds} rounds.")
                    break
            else:
                no_new_rounds = 0

        # --- L∆∞u CSV (append, ch·ªâ link m·ªõi) ---
        file_exists = os.path.exists(output_csv)
        with open(output_csv, 'a', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            if not file_exists:
                writer.writerow(['url'])
            # Ghi theo set difference, c√≥ sort ƒë·ªÉ ·ªïn ƒë·ªãnh
            new_only_sorted = sorted(collected)
            for link in new_only_sorted:
                writer.writerow([link])

        print(f"‚úÖ Added {len(new_only_sorted)} new links. "
              f"Total (existing+new): {len(existing) + len(new_only_sorted)}. Saved to {output_csv}")

    finally:
        driver.quit()

def crawl_infinite_links_2(
    category_url: str,
    css_selector: str = 'a.box-category-link-with-avatar.img-resize',
    output_csv: str = 'thanhnien_links.csv',
    max_scrolls: int = 60,          # v·∫´n gi·ªØ cho tr∆∞·ªùng h·ª£p kh√¥ng d√πng th·ªùi gian
    pause_sec: float = 1.2,
    stop_if_no_new_rounds: int = 3,
    headless: bool = True,
    try_click_selectors: tuple = (
        'a.view-more.btn-viewmore[href="javascript:;"]',
    ),
    # >>> TH√äM 3 THAM S·ªê M·ªöI <<<
    max_minutes: float | None = None,    # n·∫øu set (vd 5.0) -> ch·∫°y theo th·ªùi gian
    idle_timeout_secs: int = 90,         # kh√¥ng c√≥ link m·ªõi qu√° X gi√¢y -> d·ª´ng
    save_every_secs: int = 60,           # ghi CSV t·∫°m m·ªói X gi√¢y
):
    # --- Chrome Options (gi·∫£m b·ªã ch·∫∑n headless) ---
    chrome_opts = Options()
    if headless:
        chrome_opts.add_argument("--headless=new")
    chrome_opts.add_argument("--disable-gpu")
    chrome_opts.add_argument("--no-sandbox")
    chrome_opts.add_argument("--disable-dev-shm-usage")
    chrome_opts.add_argument("--window-size=1280,2600")
    chrome_opts.add_argument("--disable-blink-features=AutomationControlled")
    chrome_opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_opts.add_experimental_option('useAutomationExtension', False)
    chrome_opts.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    )

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=chrome_opts
    )

    # --- Load links c≈© (tr√°nh tr√πng) ---
    existing = set()
    if os.path.exists(output_csv):
        with open(output_csv, encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                existing.add(row['url'])
        print(f"üîπ Loaded {len(existing)} existing links from {output_csv}")

    collected = set()
    no_new_rounds = 0

    # >>> TH√äM: bi·∫øn theo d√µi th·ªùi gian <<<
    start_ts = time.time()
    last_new_ts = start_ts
    last_save_ts = start_ts

    try:
        driver.get(category_url)
        time.sleep(2)

        # ƒë√≥ng consent (gi·ªØ nguy√™n)
        for sel in ['button#onetrust-accept-btn-handler', '.ot-pc-refuse-all-handler',
                    'button[aria-label="accept"]', 'button:contains("ƒê·ªìng √Ω")']:
            try:
                for el in driver.find_elements(By.CSS_SELECTOR, sel):
                    if el.is_displayed() and el.is_enabled():
                        driver.execute_script("arguments[0].click();", el)
                        time.sleep(0.5)
            except Exception:
                pass

        last_height = driver.execute_script("return document.body.scrollHeight")

        # >>> N·∫æU C√ì max_minutes -> d√πng while theo th·ªùi gian; ng∆∞·ª£c l·∫°i d√πng for nh∆∞ c≈© <<<
        round_idx = 0
        def time_left():
            return (max_minutes is None) or (time.time() - start_ts < max_minutes * 60)

        while time_left() if max_minutes is not None else round_idx < max_scrolls:
            round_idx += 1

            # 1) click n√∫t xem th√™m (n·∫øu c√≥)
            for sel in try_click_selectors:
                try:
                    for btn in driver.find_elements(By.CSS_SELECTOR, sel):
                        if btn.is_displayed() and btn.is_enabled():
                            try:
                                driver.execute_script("arguments[0].click();", btn)
                                time.sleep(pause_sec)
                            except WebDriverException:
                                try:
                                    btn.click(); time.sleep(pause_sec)
                                except Exception:
                                    pass
                except Exception:
                    pass

            # 2) Cu·ªôn k√≠ch ho·∫°t lazy-load
            for _ in range(5):
                driver.execute_script("window.scrollBy(0, Math.floor(window.innerHeight*0.9));")
                time.sleep(pause_sec/3)

            # 3) Cu·ªôn t·ªõi ƒë√°y
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(pause_sec)

            # 4) Ki·ªÉm tra tƒÉng chi·ªÅu cao
            new_height = driver.execute_script("return document.body.scrollHeight")
            height_grew = new_height > last_height
            last_height = new_height

            # 5) Parse & gom link
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            before = len(collected)
            for a in soup.select(css_selector):
                href = a.get('href')
                if not href:
                    continue
                full = urljoin(category_url, href)                
                if (full not in existing) and (full not in collected):
                    collected.add(full)
            new_links = len(collected) - before

            # c·∫≠p nh·∫≠t ‚Äúc√≥ ti·∫øn tri·ªÉn‚Äù
            if new_links > 0:
                last_new_ts = time.time()
                no_new_rounds = 0
            else:
                no_new_rounds += 1

            print(f"üîπ Round {round_idx}: +{new_links} new, total={len(existing) + len(collected)} | height_grew={height_grew}")

            # 6) Ghi CSV t·∫°m theo chu k·ª≥ (tu·ª≥ ch·ªçn)
            now = time.time()
            if save_every_secs and (now - last_save_ts >= save_every_secs) and len(collected) > 0:
                file_exists = os.path.exists(output_csv)
                with open(output_csv, 'a', encoding='utf-8-sig', newline='') as f:
                    writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                    if not file_exists:
                        writer.writerow(['url'])
                    for link in sorted(collected):
                        writer.writerow([link])
                existing.update(collected)
                collected.clear()
                last_save_ts = now
                print(f"üíæ Saved interim batch. Total saved so far: {len(existing)}")

            # 7) ƒêi·ªÅu ki·ªán d·ª´ng
            if max_minutes is None:
                # ch·∫ø ƒë·ªô c≈©: d·ª´ng theo v√≤ng + chi·ªÅu cao
                if new_links == 0 and not height_grew:
                    if no_new_rounds >= stop_if_no_new_rounds:
                        print(f"‚ÑπÔ∏è  Stop: no progress for {no_new_rounds} rounds.")
                        break
            else:
                # ch·∫ø ƒë·ªô theo th·ªùi gian: n·∫øu kh√¥ng c√≥ link m·ªõi qu√° idle_timeout_secs -> d·ª´ng
                if now - last_new_ts >= idle_timeout_secs:
                    print(f"‚ÑπÔ∏è  Stop: idle {int(now - last_new_ts)}s without new links (time-mode).")
                    break

        # --- L∆∞u ph·∫ßn c√≤n l·∫°i (n·∫øu c√≥) ---
        if len(collected) > 0:
            file_exists = os.path.exists(output_csv)
            with open(output_csv, 'a', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                if not file_exists:
                    writer.writerow(['url'])
                for link in sorted(collected):
                    writer.writerow([link])

        print(f"‚úÖ Done. Appended new links. Check {output_csv}")

    finally:
        driver.quit()



# ============================
# üëâ C√°ch s·ª≠ d·ª•ng
# ============================
if __name__ == "__main__":
    # # Danh s√°ch chuy√™n m·ª•c c·∫ßn crawl
    # categories = [
    #     # "https://thanhnien.vn/kinh-te.htm",
    #     # "https://thanhnien.vn/thoi-su/phap-luat.htm",
    #     "https://thanhnien.vn/the-thao.htm",
    # ]

    # # Ch·∫°y l·∫ßn l∆∞·ª£t t·ª´ng chuy√™n m·ª•c
    # for url in categories:
    #     print(f"\nüåê Crawling category: {url}")
    #     crawl_infinite_links(
    #         category_url=url,
    #         css_selector='a.box-category-link-with-avatar',
    #         output_csv='thanhnien_links.csv',
    #         max_scrolls=20,
    #         pause_sec=1.2,
    #         stop_if_no_new_rounds=5,
    #         headless=False
    #     )
       
    crawl_infinite_links_2(
        category_url="https://thanhnien.vn/the-thao.htm",
        css_selector="a.box-category-link-title",
        output_csv="thanhnien_links.csv",
        headless=False,
        max_minutes=15.0,          # ‚¨ÖÔ∏è ch·∫°y ƒë√∫ng 15 ph√∫t (tr·ª´ khi idle qu√° l√¢u)
        idle_timeout_secs=600,     # kh√¥ng c√≥ link m·ªõi > 600s th√¨ tho√°t s·ªõm
        save_every_secs=60,        # ghi t·∫°m m·ªói 60s        
    )