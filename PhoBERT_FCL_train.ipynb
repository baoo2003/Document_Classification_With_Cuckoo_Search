{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa056437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhoBERT import PhoBERTTextPreprocessor, VietnameseNewsDataset, PhoBERTClassifier, PhoBERTTrainer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DataCollatorWithPadding, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: False\n",
      "WARNING: No GPU detected! Training will be slow on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra GPU\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"Notebook n√†y y√™u c·∫ßu GPU ƒë·ªÉ ch·∫°y. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh CUDA.\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f1dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading PhoBERT tokenizer from local: models/phobert-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 206,411\n",
      "Validation samples: 25,967\n",
      "Number of classes: 13\n",
      "Classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "preprocessor_train = PhoBERTTextPreprocessor(\n",
    "        phobert_model='vinai/phobert-base',\n",
    "        max_length=32\n",
    ")\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = VietnameseNewsDataset(\n",
    "    csv_file='data/preprocess/UIT-ViON_train_preprocessed.csv',\n",
    "    preprocessor=preprocessor_train,\n",
    "    max_header_length=20\n",
    ")\n",
    "\n",
    "# Load validation dataset (s·ª≠ d·ª•ng label_encoder t·ª´ train)\n",
    "val_dataset = VietnameseNewsDataset(\n",
    "    csv_file='data/preprocess/UIT-ViON_dev_preprocessed.csv',\n",
    "    preprocessor=preprocessor_train,\n",
    "    label_encoder=train_dataset.label_encoder,\n",
    "    max_header_length=20\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"Number of classes: {len(train_dataset.label_encoder.classes_)}\")\n",
    "print(f\"Classes: {train_dataset.label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5590a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Training on CPU - Applying optimizations...\n",
      "\n",
      "Configuration Summary:\n",
      "  Device:              CPU\n",
      "  Batch size:          16\n",
      "  Effective batch:     32\n",
      "  Num workers:         2\n",
      "  Pin memory:          False\n",
      "  Training batches:    12,901\n",
      "  Validation batches:  812\n",
      "\n",
      "CPU Training Tips:\n",
      "  - Training will be 10-20x slower than GPU\n",
      "  - Estimated: ~15-20 minutes/epoch with 206K samples\n",
      "  - Consider reducing NUM_EPOCHS to 2\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# GPU Configuration\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "\n",
    "print(f\"‚úÖ Training on GPU: {gpu_name}\")\n",
    "print(f\"   GPU Memory: {gpu_memory_gb:.1f} GB\\n\")\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "# c√°c tham s·ªë c√≥ th√™ t·ªëi ∆∞u\n",
    "BATCH_SIZE = 16 # [16, 24, 32]\n",
    "DROPOUT_RATE = 0.4 # [0.1, 0.4, 0.5]\n",
    "# ========================================\n",
    "# HYPERPARAMETERS\n",
    "# ========================================\n",
    "LEARNING_RATE = 2e-5 #[1e-5, 2e-5, 3e-5, 5e-5]\n",
    "WEIGHT_DECAY = 0.01 # (0.0 -> 0.1)\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "ACCUMULATION_STEPS = 1\n",
    "\n",
    "USE_PIN_MEMORY = True\n",
    "PERSISTENT_WORKERS = True if NUM_WORKERS > 0 else False\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=preprocessor_train.tokenizer)\n",
    "\n",
    "# DataLoader configuration\n",
    "dataloader_kwargs = {\n",
    "    'collate_fn': data_collator,\n",
    "    'pin_memory': USE_PIN_MEMORY,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'persistent_workers': PERSISTENT_WORKERS,\n",
    "    'prefetch_factor': PREFETCH_FACTOR\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"DATALOADER CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Device:              {device.upper()}\")\n",
    "print(f\"  Batch size:          {BATCH_SIZE}\")\n",
    "print(f\"  Accumulation steps:  {ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective batch:     {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
    "print(f\"  Num workers:         {NUM_WORKERS}\")\n",
    "print(f\"  Training batches:    {len(train_loader):,}\")\n",
    "print(f\"  Validation batches:  {len(val_loader):,}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nüöÄ GPU Training Optimizations Enabled:\")\n",
    "print(f\"  ‚Ä¢ Mixed precision training (FP16)\")\n",
    "print(f\"  ‚Ä¢ Pin memory for faster data transfer\")\n",
    "print(f\"  ‚Ä¢ Persistent workers to reduce overhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e156e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initialize Model & Trainer ===\n",
      ">> Loading PhoBERT model from local: models/phobert-base\n",
      "Mode: Full fine-tuning\n",
      "Trainable: 135,008,269 parameters\n",
      "\n",
      "Using device: cpu\n",
      "WARNING: No GPU detected! Training will be slow.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Initialize Model & Trainer ===\")\n",
    "\n",
    "# ========================================\n",
    "# MODEL CONFIGURATION\n",
    "# ========================================\n",
    "FREEZE_PHOBERT = False\n",
    "    \n",
    "# Initialize model\n",
    "num_classes = len(train_dataset.label_encoder.classes_)\n",
    "model_train = PhoBERTClassifier(\n",
    "    num_classes=num_classes,\n",
    "    phobert_model='vinai/phobert-base',\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    hidden_size=768,\n",
    "    freeze_phobert=FREEZE_PHOBERT\n",
    ")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_train.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_train.parameters())\n",
    "\n",
    "if FREEZE_PHOBERT:\n",
    "    print(f\"Mode: Freeze PhoBERT (train FC layer only)\")\n",
    "    print(f\"Trainable: {trainable_params:,} / {total_params:,} parameters\")\n",
    "else:\n",
    "    print(f\"Mode: Full fine-tuning\")\n",
    "    print(f\"Trainable: {trainable_params:,} parameters\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = PhoBERTTrainer(\n",
    "    model=model_train,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d46291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Setup Optimizer & Learning Rate ===\n",
      "Learning rate: 2e-05\n",
      "Weight decay: 0.05\n",
      "Number of epochs: 3\n",
      "Total training steps: 19,351\n",
      "Warmup steps: 1,935\n",
      "Gradient accumulation: 2\n",
      "Estimated training time: 45-60 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Setup Optimizer & Learning Rate ===\")\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model_train.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': WEIGHT_DECAY\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model_train.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = NUM_EPOCHS * len(train_loader) // ACCUMULATION_STEPS\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Total training steps: {total_steps:,}\")\n",
    "print(f\"Warmup steps: {warmup_steps:,}\")\n",
    "print(f\"Gradient accumulation: {ACCUMULATION_STEPS}\")\n",
    "print(f\"Scheduler: Cosine Annealing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb852172",
   "metadata": {},
   "source": [
    "## üöÄ Advanced Improvements Applied\n",
    "\n",
    "ƒê·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh v√† gi·∫£m thi·ªÉu overfitting t·ªët h∆°n n·ªØa, t√¥i ƒë√£ √°p d·ª•ng c√°c k·ªπ thu·∫≠t n√¢ng cao sau:\n",
    "\n",
    "1.  **Label Smoothing (0.1):**\n",
    "    *   Thay v√¨ √©p m√¥ h√¨nh d·ª± ƒëo√°n x√°c su·∫•t 1.0 cho ƒë√∫ng class (hard target), ch√∫ng ta s·ª≠ d·ª•ng soft target (v√≠ d·ª•: 0.9 cho ƒë√∫ng class, chia ƒë·ªÅu 0.1 cho c√°c class c√≤n l·∫°i).\n",
    "    *   **T√°c d·ª•ng:** Gi√∫p m√¥ h√¨nh b·ªõt \"t·ª± tin th√°i qu√°\", h·ªçc ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng t·ªïng qu√°t h∆°n v√† gi·∫£m overfitting.\n",
    "\n",
    "2.  **Correct Weight Decay Strategy:**\n",
    "    *   Ch·ªâ √°p d·ª•ng Weight Decay cho c√°c tr·ªçng s·ªë (weights) c·ªßa Linear layers v√† Embeddings.\n",
    "    *   **KH√îNG** √°p d·ª•ng cho Bias v√† LayerNorm weights.\n",
    "    *   **T√°c d·ª•ng:** ƒê√¢y l√† chu·∫©n m·ª±c khi fine-tune BERT, gi√∫p training ·ªïn ƒë·ªãnh h∆°n.\n",
    "\n",
    "3.  **Cosine Learning Rate Scheduler:**\n",
    "    *   Thay v√¨ gi·∫£m tuy·∫øn t√≠nh (Linear), LR s·∫Ω gi·∫£m theo h√¨nh cosin.\n",
    "    *   **T√°c d·ª•ng:** Gi·ªØ LR cao l√¢u h∆°n ·ªü giai ƒëo·∫°n ƒë·∫ßu ƒë·ªÉ h·ªçc nhanh, v√† gi·∫£m r·∫•t ch·∫≠m/m∆∞·ª£t v·ªÅ cu·ªëi ƒë·ªÉ h·ªôi t·ª• ch√≠nh x√°c v√†o ƒëi·ªÉm c·ª±c tr·ªã.\n",
    "\n",
    "4.  **Hyperparameters Adjustment:**\n",
    "    *   **Epochs:** TƒÉng nh·∫π l√™n 4 ƒë·ªÉ Cosine Scheduler c√≥ ƒë·ªß chu k·ª≥ ho·∫°t ƒë·ªông hi·ªáu qu·∫£.\n",
    "    *   **Weight Decay:** ƒêi·ªÅu ch·ªânh v·ªÅ 0.01 (chu·∫©n cho AdamW khi ƒë√£ group parameters ƒë√∫ng c√°ch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "START TRAINING ON CPU\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EPOCH 1/3\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|         | 16/12901 [01:47<24:04:08,  6.72s/it, loss=2.8804, acc=0.0664, lr=8.27e-08]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# TRAINING\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m train_loss, train_acc = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mACCUMULATION_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_mixed_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_MIXED_PRECISION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# VALIDATION\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating on validation set...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/VscodeProjects/Document_Classification_With_Cuckoo_Search/PhoBERT.py:277\u001b[39m, in \u001b[36mPhoBERTTrainer.train_epoch\u001b[39m\u001b[34m(self, train_loader, optimizer, scheduler, accumulation_steps, use_mixed_precision, max_grad_norm, show_progress)\u001b[39m\n\u001b[32m    275\u001b[39m     scaler.scale(loss).backward()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# Update weights sau m·ªói accumulation_steps\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx + \u001b[32m1\u001b[39m) % accumulation_steps == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/VscodeProjects/Document_Classification_With_Cuckoo_Search/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/VscodeProjects/Document_Classification_With_Cuckoo_Search/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/VscodeProjects/Document_Classification_With_Cuckoo_Search/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"START TRAINING ON CUDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "\n",
    "# ========================================\n",
    "# TRAINING LOOP WITH EARLY STOPPING\n",
    "# ========================================\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "total_training_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EPOCH {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # TRAINING\n",
    "    train_loss, train_acc = trainer.train_epoch(\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        accumulation_steps=ACCUMULATION_STEPS,\n",
    "        use_mixed_precision=True, # Always True for CUDA\n",
    "        max_grad_norm=1.0,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # VALIDATION\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_loss, val_acc = trainer.evaluate(\n",
    "        val_loader=val_loader,\n",
    "        show_progress=True,\n",
    "        use_mixed_precision=True\n",
    "    )\n",
    "\n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Print results\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"\\nEPOCH {epoch + 1} RESULTS:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    print(f\"  Gap:        Loss delta={abs(train_loss-val_loss):.4f} | Acc delta={abs(train_acc-val_acc)*100:.2f}%\")\n",
    "    print(f\"  Time:       {epoch_time/60:.2f} minutes ({epoch_time:.0f} seconds)\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    # EARLY STOPPING\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_train.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'label_encoder': train_dataset.label_encoder,\n",
    "            'history': history,\n",
    "            'config': {\n",
    "                'num_classes': num_classes,\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'num_epochs': NUM_EPOCHS,\n",
    "                'dropout_rate': 0.5,\n",
    "                'weight_decay': WEIGHT_DECAY,\n",
    "                'device': 'cuda'\n",
    "            }\n",
    "        }\n",
    "        torch.save(checkpoint, 'best_phobert_model.pth')\n",
    "        print(f\"  >> Saved best model (Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        improvement = ((best_val_loss - val_loss) / best_val_loss) * 100\n",
    "        print(f\"  >> Val loss did NOT improve (increased {-improvement:.2f}%) - Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"EARLY STOPPING ACTIVATED\")\n",
    "            print(f\"Val loss did not improve after {patience} epochs\")\n",
    "            print(f\"Best Val Loss: {best_val_loss:.4f} (Epoch {epoch - patience_counter + 1})\")\n",
    "            print(f\"Stopping training to prevent overfitting\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            break\n",
    "\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "total_training_time = time.time() - total_training_start\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: CUDA\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"Total training time: {total_training_time/60:.1f} minutes ({total_training_time:.0f} seconds)\")\n",
    "print(f\"Average time per epoch: {total_training_time/len(history['train_loss'])/60:.1f} minutes\")\n",
    "print(f\"Total epochs completed: {len(history['train_loss'])}/{NUM_EPOCHS}\")\n",
    "print(f\"Model saved at: best_phobert_model.pth\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfedf39",
   "metadata": {},
   "source": [
    "## Overfitting Prevention Measures\n",
    "\n",
    "**4 Implemented Actions:**\n",
    "\n",
    "1. **Early Stopping (patience=2)**\n",
    "   - Stop training when val_loss doesn't improve after 2 epochs\n",
    "   - Save model based on val_loss (not val_acc)\n",
    "   - Display detailed information when stopping\n",
    "\n",
    "2. **Increased Dropout: 0.3 ‚Üí 0.5**\n",
    "   - Stronger regularization for FC layer\n",
    "   - Reduce overfitting by random dropping neurons\n",
    "\n",
    "3. **Increased Weight Decay: 0.01 ‚Üí 0.05**\n",
    "   - Stronger L2 regularization\n",
    "   - Penalty large weights to avoid overly complex model\n",
    "\n",
    "4. **Reduced NUM_EPOCHS: 5 ‚Üí 3**\n",
    "   - Reduce training time\n",
    "   - Combined with early stopping to stop at right time\n",
    "\n",
    "**Expected Results:**\n",
    "- Val loss decreases or stabilizes (NOT increase like before)\n",
    "- Gap between train/val < 5%\n",
    "- Better generalization on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot Loss\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy\n",
    "ax2.plot([acc*100 for acc in history['train_acc']], label='Train Acc', marker='o')\n",
    "ax2.plot([acc*100 for acc in history['val_acc']], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training & Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Best epoch: {history['val_acc'].index(max(history['val_acc'])) + 1}\")\n",
    "print(f\"  Best val accuracy: {max(history['val_acc']):.4f} ({max(history['val_acc'])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e37e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "checkpoint = torch.load('best_phobert_model.pth', weights_only=False)\n",
    "model_train.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1} with val_acc={checkpoint['val_acc']:.4f}\")\n",
    "\n",
    "# Predict on validation set\n",
    "print(\"\\nPredicting on validation set...\")\n",
    "y_pred, y_true = trainer.predict_from_loader(val_loader)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=train_dataset.label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"-\" * 80)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=train_dataset.label_encoder.classes_,\n",
    "    yticklabels=train_dataset.label_encoder.classes_\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d827351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "checkpoint = torch.load('best_phobert_model.pth', weights_only=False)\n",
    "model_train.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1} with val_acc={checkpoint['val_acc']:.4f}\")\n",
    "\n",
    "# Test with sample texts\n",
    "test_texts = [\n",
    "    \"√¥_nhi·ªÖm ti·∫øng ·ªìn √¢m_th·∫ßm ti√™u_di·ªát sinh_v·∫≠t bi·ªÉn\",\n",
    "    \"to√†n_c·∫ßu nu√¥i 8 t·ª∑ mi·ªáng_ƒÉn trong ƒë·∫°i_d·ªãch nh∆∞_th·∫ø_n√†o\",\n",
    "    \"th√°ch_th·ª©c lao_ƒë·ªông vi·ªát ƒë√≥n s√≥ng chuy·ªÉn_d·ªãch nh·∫≠t_b·∫£n\",\n",
    "    \"vi·ªát_trinh l√†m vedette\",\n",
    "    \"m·ªπ b·∫≠t_ƒë√®n_xanh b√°n m√°y_bay chi·∫øn_ƒë·∫•u hi·ªán_ƒë·∫°i f 35 singapore\",\n",
    "    \"ch·∫∑t ch√©m ti·ªÅn g·ª≠i xe m√πa ph√°o_hoa g·ªçi ai\",\n",
    "    \"15 ti√™u_ch√≠ ƒë√°nh_gi√° an_to√†n ph√≤ng covid 19 ƒë·ªëi_v·ªõi tr∆∞·ªùng_h·ªçc\"\n",
    "]\n",
    "\n",
    "print(\"\\nTest predictions:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "predictions = trainer.predict(\n",
    "    texts=test_texts,\n",
    "    preprocessor=preprocessor_train,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "label_encoder = checkpoint['label_encoder']\n",
    "for i, (text, pred_idx) in enumerate(zip(test_texts, predictions), 1):\n",
    "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   Predicted: {pred_label}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Download model (optional)\n",
    "print(\"\\nDownload model:\")\n",
    "print(\"  Uncomment the lines below to download:\")\n",
    "print(\"  # from google.colab import files\")\n",
    "print(\"  # files.download('best_phobert_model.pth')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
